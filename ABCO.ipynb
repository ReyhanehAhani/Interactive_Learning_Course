{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "import pygame\n",
        "import matplotlib.pyplot as plt  # Import matplotlib for plotting\n",
        "\n",
        "# Define your PongEnvironment class\n",
        "class PongEnvironment:\n",
        "    ACTION_SPACE = [0, 1]  # Define the action space\n",
        "    def __init__(self, width=400, height=300):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.ball_radius = 10\n",
        "        self.paddle_width = 10\n",
        "        self.paddle_height = 60\n",
        "        self.paddle_offset = 20\n",
        "        self.ball_pos = np.array([self.width // 2, self.height // 2], dtype=float)\n",
        "        self.ball_vel = np.array([0.03, 0.01], dtype=float)\n",
        "        self.paddle_pos = self.height // 2\n",
        "        self.clock = pygame.time.Clock()\n",
        "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
        "        pygame.display.set_caption(\"Pong\")\n",
        "\n",
        "    def generate_expert_transitions(self, num_transitions=100):\n",
        "        transitions = []\n",
        "\n",
        "        for _ in range(num_transitions):\n",
        "            action = np.random.choice(self.ACTION_SPACE)\n",
        "            state = self.get_state()\n",
        "\n",
        "            # Execute action in the environment\n",
        "            self.move_paddle(action)\n",
        "            next_state = self.get_state()\n",
        "\n",
        "            # Determine reward and done\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "            # Check if the ball hits the paddle\n",
        "            if self.ball_pos[0] >= self.width - self.ball_radius - self.paddle_width:\n",
        "                if self.paddle_pos - self.paddle_height / 2 <= self.ball_pos[1] <= self.paddle_pos + self.paddle_height / 2:\n",
        "                    reward = 2  # Reward for hitting the ball\n",
        "                else:\n",
        "                    reward = -1  # Penalty for missing the ball\n",
        "                    done = True\n",
        "\n",
        "            transitions.append((state, action, reward, next_state, done))\n",
        "\n",
        "        return transitions\n",
        "\n",
        "    def reset(self):\n",
        "        self.ball_pos = np.array([self.width // 2, self.height // 2], dtype=float)\n",
        "        self.ball_vel = np.array([0.03, 0.01], dtype=float)\n",
        "        self.paddle_pos = self.height // 2\n",
        "\n",
        "    def step(self, action):\n",
        "        self.move_paddle(action)\n",
        "        self.ball_pos += self.ball_vel\n",
        "\n",
        "        if self.ball_pos[1] <= self.ball_radius or self.ball_pos[1] >= self.height - self.ball_radius:\n",
        "            self.ball_vel[1] *= -1\n",
        "\n",
        "        if self.ball_pos[0] <= self.ball_radius:\n",
        "            return self.get_state(), -1, True  # Reward -1 for missing the ball, set done to True\n",
        "\n",
        "        if self.ball_pos[0] >= self.width - self.ball_radius:\n",
        "            if self.paddle_pos - self.paddle_height / 2 <= self.ball_pos[1] <= self.paddle_pos + self.paddle_height / 2:\n",
        "                self.ball_vel[0] *= -1\n",
        "            else:\n",
        "                return self.get_state(), -1, True  # Reward -1 for missing the ball, set done to True\n",
        "\n",
        "        return self.get_state(), 0, False\n",
        "\n",
        "    def move_paddle(self, action):\n",
        "        self.paddle_pos = np.clip(self.paddle_pos + action, self.paddle_height / 2, self.height - self.paddle_height / 2)\n",
        "\n",
        "    def get_state(self):\n",
        "        return np.array([\n",
        "            self.ball_pos[0] / self.width,\n",
        "            self.ball_pos[1] / self.height,\n",
        "            self.ball_vel[0],\n",
        "            self.ball_vel[1],\n",
        "            self.paddle_pos / self.height\n",
        "        ], dtype=np.float32)  # Ensure consistent data type\n",
        "\n",
        "    def render(self):\n",
        "        self.screen.fill((0, 0, 0))\n",
        "        pygame.draw.rect(self.screen, (255, 255, 255), pygame.Rect(0, self.paddle_pos - self.paddle_height / 2, self.paddle_width, self.paddle_height))\n",
        "        pygame.draw.circle(self.screen, (255, 255, 255), (int(self.ball_pos[0]), int(self.ball_pos[1])), self.ball_radius)\n",
        "        pygame.display.flip()\n",
        "\n",
        "# Define the neural network for approximating the model MÎ¸\n",
        "class ModelApproximator(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=64):\n",
        "        super(ModelApproximator, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, action_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Define the DQN class\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=64):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, action_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Define the ReplayBuffer class\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        if state is not None and next_state is not None:\n",
        "            self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if len(self.buffer) == 0:\n",
        "            return None, None, None, None, None\n",
        "\n",
        "        if len(self.buffer) < batch_size:\n",
        "            batch_size = len(self.buffer)\n",
        "\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
        "        return torch.FloatTensor(state), torch.LongTensor(action), torch.FloatTensor(reward), torch.FloatTensor(next_state), torch.FloatTensor(done)\n",
        "\n",
        "# Define the ExpertAgent class\n",
        "class ExpertAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.model = ModelApproximator(state_size=env.get_state().shape[0], action_size=len(env.ACTION_SPACE))\n",
        "        self.replay_buffer = ReplayBuffer(capacity=10000)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)  # Define optimizer\n",
        "\n",
        "    def train(self, num_episodes, batch_size):\n",
        "        rewards_per_episode = []\n",
        "        losses = []\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            state = self.env.reset()\n",
        "            total_reward = 0\n",
        "            episode_losses = []\n",
        "\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.select_action(state)\n",
        "                next_state, reward, done = self.env.step(action)\n",
        "                self.replay_buffer.add(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "                if len(self.replay_buffer.buffer) >= batch_size:\n",
        "                    states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
        "                    loss = self.train_step(states, actions, rewards, next_states, dones)\n",
        "                    episode_losses.append(loss)\n",
        "\n",
        "            rewards_per_episode.append(total_reward)\n",
        "            losses.append(np.mean(episode_losses) if episode_losses else 0)\n",
        "            print(f'Expert Agent - Episode {episode}, Total Reward: {total_reward}, Average Loss: {np.mean(episode_losses) if episode_losses else 0}')\n",
        "\n",
        "        return rewards_per_episode, losses\n",
        "\n",
        "    def test(self, num_episodes):\n",
        "        rewards_per_episode = []\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            state = self.env.reset()\n",
        "            total_reward = 0\n",
        "\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.select_action(state)\n",
        "                next_state, reward, done = self.env.step(action)\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "            rewards_per_episode.append(total_reward)\n",
        "            print(f'Expert Agent - Test Episode {episode}, Total Reward: {total_reward}')\n",
        "\n",
        "        return rewards_per_episode\n",
        "\n",
        "    def select_action(self, state, epsilon=0.1):\n",
        "        if state is None:  # Handle None state\n",
        "            return random.choice(range(len(self.env.ACTION_SPACE)))\n",
        "\n",
        "        if random.random() > epsilon:\n",
        "            with torch.no_grad():\n",
        "                state_t = torch.FloatTensor(state).unsqueeze(0)\n",
        "                action_values = self.model(state_t)\n",
        "                action = np.argmax(action_values.cpu().numpy())\n",
        "        else:\n",
        "            action = random.choice(range(len(self.env.ACTION_SPACE)))\n",
        "        return action\n",
        "\n",
        "\n",
        "    def train_step(self, states, actions, rewards, next_states, dones):\n",
        "        state_action_values = self.model(states).gather(1, actions.view(-1, 1))\n",
        "        next_state_values = self.model(next_states).max(1)[0].detach()\n",
        "        expected_state_action_values = (next_state_values * 0.99 * (1 - dones)) + rewards\n",
        "        loss = nn.MSELoss()(state_action_values.squeeze(), expected_state_action_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def get_observations(self, num_samples=10):\n",
        "        observations = []\n",
        "        for _ in range(num_samples):\n",
        "            state = self.env.reset()\n",
        "            action = random.choice(self.env.ACTION_SPACE)\n",
        "            action = random.choice(self.env.ACTION_SPACE)\n",
        "            next_state, reward, done = self.env.step(action)\n",
        "            observations.append((state, action, reward, next_state, done))\n",
        "        return observations\n",
        "\n",
        "# Define the NonExpertAgent class\n",
        "class NonExpertAgent:\n",
        "    def __init__(self, env, expert_agents):\n",
        "        self.env = env\n",
        "        self.model = ModelApproximator(state_size=env.get_state().shape[0], action_size=len(env.ACTION_SPACE))\n",
        "        self.replay_buffer = ReplayBuffer(capacity=10000)\n",
        "        self.expert_agents = expert_agents\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)  # Initialize optimizer\n",
        "\n",
        "    def train(self, num_episodes, batch_size):\n",
        "        rewards_per_episode = []\n",
        "        losses = []\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            state = self.env.reset()\n",
        "            total_reward = 0\n",
        "            episode_losses = []\n",
        "\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.select_action(state)\n",
        "                next_state, reward, done = self.env.step(action)\n",
        "                self.replay_buffer.add(state, action, reward, next_state, done)\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "                if len(self.replay_buffer.buffer) >= batch_size:\n",
        "                    self.collect_from_experts(batch_size)\n",
        "                    states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
        "                    state_action_values = self.model(states)\n",
        "                    actions = actions.type(torch.LongTensor)  # Convert actions to LongTensor\n",
        "                    loss = criterion(state_action_values, actions)  # Use CrossEntropyLoss for classification\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "                    episode_losses.append(loss.item())\n",
        "\n",
        "            rewards_per_episode.append(total_reward)\n",
        "            losses.append(np.mean(episode_losses) if episode_losses else 0)\n",
        "            print(f'Non-Expert Agent - Episode {episode}, Total Reward: {total_reward}, Average Loss: {np.mean(episode_losses) if episode_losses else 0}')\n",
        "\n",
        "        return rewards_per_episode, losses\n",
        "\n",
        "    def collect_from_experts(self, batch_size):\n",
        "        for expert_agent in self.expert_agents:\n",
        "            observations = expert_agent.get_observations(batch_size)\n",
        "            for state, _, reward, next_state, done in observations:\n",
        "                self.replay_buffer.add(state, None, reward, next_state, done)\n",
        "\n",
        "\n",
        "    def select_action(self, state, epsilon=0.1):\n",
        "        if state is None:  # Handle None state\n",
        "            return random.choice(range(len(self.env.ACTION_SPACE)))\n",
        "\n",
        "        if random.random() > epsilon:\n",
        "            with torch.no_grad():\n",
        "                state_t = torch.FloatTensor(state).unsqueeze(0)\n",
        "                action_values = self.model(state_t)\n",
        "                action = np.argmax(action_values.cpu().numpy())\n",
        "        else:\n",
        "            action = random.choice(range(len(self.env.ACTION_SPACE)))\n",
        "        return action\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    pygame.init()\n",
        "    env = PongEnvironment()\n",
        "    expert_agents = [ExpertAgent(env) for _ in range(3)]  # Create multiple expert agents\n",
        "    non_expert_agent = NonExpertAgent(env, expert_agents)\n",
        "\n",
        "    num_episodes = 100\n",
        "    batch_size = 32\n",
        "\n",
        "    # Train expert agents\n",
        "    for expert_agent in expert_agents:\n",
        "        expert_agent.train(num_episodes, batch_size)\n",
        "\n",
        "    # Train non-expert agent\n",
        "    non_expert_agent.train(num_episodes, batch_size)\n",
        "\n",
        "    pygame.quit()\n",
        "\n",
        "    # Plot rewards per episode for expert agents\n",
        "    for i, expert_agent in enumerate(expert_agents):\n",
        "        rewards_per_episode = expert_agent.test(num_episodes)\n",
        "        plt.plot(rewards_per_episode, label=f'Expert Agent {i+1}')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.title('Rewards per Episode for Expert Agents')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Plot rewards per episode for non-expert agent\n",
        "    rewards_per_episode, _ = non_expert_agent.test(num_episodes)\n",
        "    plt.plot(rewards_per_episode, label='Non-Expert Agent')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.title('Rewards per Episode for Non-Expert Agent')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ri1rqcvUwQoy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}